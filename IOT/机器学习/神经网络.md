## 感知机
假设训练集为![](https://cdn.nlark.com/yuque/__latex/cbb3e50d25b26c602d1dfc2fef867f37.svg#card=math&code=D%3D%5C%7B%28x_i%2Cy_i%29%5C%7D%5E%7Bm%7D_%7Bi%3D1%7D&id=AaudU)且**线性可分**，其中，![](https://cdn.nlark.com/yuque/__latex/c6af5a0c243f8acf778dd5dd5ce5dadb.svg#card=math&code=x_i%E2%88%88X%E2%8A%86R%5En&id=h4ZxN)，![](https://cdn.nlark.com/yuque/__latex/01e35aaa019e3074526bba6723093edd.svg#card=math&code=y_i%E2%88%88Y%3D%7B%2B1%2C-1%7D&id=YnT94)。
感知机模型为：
![](https://cdn.nlark.com/yuque/__latex/e3dee59a8ec6122b7c467c29516a97b0.svg#card=math&code=y%20%3D%20f%28%5Csum%5E%7Bn%7D_%7Bi%3D1%7Dw_ix_i-%CE%B8%29&id=mpaCK)
![](https://cdn.nlark.com/yuque/__latex/3818937ac554603003e6727783932e9f.svg#card=math&code=f%28x%29&id=dSU3m)为激活函数，通常选中符号函数或者`Sigmod`函数。
在感知机模型空间中的一个超平面可以表示为：![](https://cdn.nlark.com/yuque/__latex/397d2c04b67575bb0b4e2c66d5924c02.svg#card=math&code=wx%2Bb%20%3D%200&id=rxijO)
其中![](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg#card=math&code=w&id=OnnQd)是超平面的法向量，![](https://cdn.nlark.com/yuque/__latex/d29c2e5f4926e5b0e9a95305650f6e54.svg#card=math&code=b&id=AHL6F)是超平面的截距。这个超平面将![](https://cdn.nlark.com/yuque/__latex/558270b7f0a90c3c286b860273d106a0.svg#card=math&code=D&id=SWUyo)分成正负两类**（+1，-1）**。对所有![](https://cdn.nlark.com/yuque/__latex/49e8b83c59e22295f7fb071c5fd8d1fa.svg#card=math&code=y_i%3D%2B1&id=jR9I6)的样本，都有，对所有![](https://cdn.nlark.com/yuque/__latex/0375bb1072d33b53207cfad3c72e9e03.svg#card=math&code=y_i%3D-1&id=Dh6WX)的样本，都有
![](https://cdn.jsdelivr.net/gh/xuezhaorong/Picgo//Source/fix-dir/Download/1/2024/06/01/20-22-08-d807c0beaf8f87ffc85844a7f7f2bd0f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240601202139-23c5f9.png#id=OPcmQ&originHeight=619&originWidth=846&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none)
需要确定感知机模型的参数![](https://cdn.nlark.com/yuque/__latex/2a2fb23b1d51fac2f310c0cfa3fad3db.svg#card=math&code=w%2Cb&id=KeSzQ)，使其能够完全正确的将正类和负类分类。
对所有正确分类的点有![](https://cdn.nlark.com/yuque/__latex/09505196f703d101c8678e95c084202c.svg#card=math&code=y_i-y%28x_i%29%3D0&id=PXne6)恒成立，而所有误分类![](https://cdn.nlark.com/yuque/__latex/6f5dde593f0bc27956e14b5eaec2ed17.svg#card=math&code=M&id=dVinf)的点![](https://cdn.nlark.com/yuque/__latex/dfa7eb6880405aa65defb99ef3f1ea97.svg#card=math&code=%7Cy_i-y%28x_i%29%7C%3D2%20&id=mUNs8)恒成立，
所以优化对象为![](https://cdn.nlark.com/yuque/__latex/ff8d992f60e001498af6c21ca0050cfd.svg#card=math&code=%7Cy_i-y%28x_i%29%7C&id=YjzuQ)最小，化为![](https://cdn.nlark.com/yuque/__latex/f3b20e1ad0f0ef58ae188b956101a592.svg#card=math&code=%28y_i-y%28x_i%29%29%5E2&id=udOK5)：
![](https://cdn.nlark.com/yuque/__latex/8b3192c526cbe189df3069158b35c3ea.svg#card=math&code=L%28w%2Cb%29%3D%28y_i-y%28x_i%29%29%5E2&id=rn7qt)，
![](https://cdn.nlark.com/yuque/__latex/403349cc9b01d878be5d565e00f906a3.svg#card=math&code=%28w%2Cb%29%5E%2A%3D%5Cunderset%7Bw%2Cb%7D%7BargminL%28w%2Cb%29%7D&id=SsxCW)
分别对![](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg#card=math&code=w&id=AmGm1)和![](https://cdn.nlark.com/yuque/__latex/d29c2e5f4926e5b0e9a95305650f6e54.svg#card=math&code=b&id=cKlCf)求偏导得：
![](https://cdn.nlark.com/yuque/__latex/63ace5fe9405f39238857bdb29c107af.svg#card=math&code=%E2%88%87_wL%28w%2Cb%29%20%3D%20%28y_i-y%28x_i%29%29x_i&id=qEzOH)
![](https://cdn.nlark.com/yuque/__latex/35b361b3bd118ef752776871a4215f35.svg#card=math&code=%E2%88%87_bL%28w%2Cb%29%20%3D%20y_i-y%28x_i%29&id=ZXd11)
若感知机对训练样例![](https://cdn.nlark.com/yuque/__latex/a969cab25db24b7b40ddd3064d3ab658.svg#card=math&code=%28x%2Cy%29&id=rw81b)预测正确即![](https://cdn.nlark.com/yuque/__latex/304908e1aa93d471329835d28656bc60.svg#card=math&code=y_i%3Dy%28x_i%29&id=JYc2L)，则感知机不会发生变化，否则将根据错误的程度进行权重调整。
代码实现：
在每次的迭代中，取`η`的步长走梯度
```python
class Perceptron:
    def __init__(self, eta=0.01, iter=100):
        # eta:步长
        # iter:迭代次数
        self.w = 0
        self.eta = eta
        self.iter = iter

    def predict(self, X):
        # 填充
        X = np.insert(X,0,1,axis=1)
        n,m = X.shape
        results = np.array([])
        for i in range(n):
            result = 1 if self.sigmoid(X[i].dot(self.w.T)) > 0.5 else -1
            results = np.append(results,[result])
        results.reshape((-1,1))
        return results



    def fit(self, X, Y):
        n, m = X.shape
        self.w = np.ones(1 + m)
        for _ in range(self.iter):
            for xi, yi in zip(X, Y):
                grad = self.eta * (yi - self.predict(np.array([xi])))
                self.w[1:] += grad * xi
                self.w[0] += grad[0]


    @staticmethod
    def sigmoid(x):
        """
        Sigmoid function.
        Input:
            x:np.array
        Return:
            y: the same shape with x
        """
        y = 1.0 / (1 + np.exp(-x))
        return y
```
## 神经网络
神经网络中最基本的成分是**神经元（Neuron)**，每个神经元与其他神经元相连，当它**兴奋**时，就会向相连的神经元发送化学物质，从而改变神经元的电位，吐过神经元的点位超过了一个**阈值（threshold）**,那么它就会被激活，即**兴奋**起来，向其他神经元发送化学物质。
![](https://cdn.jsdelivr.net/gh/xuezhaorong/Picgo//Source/fix-dir/Download/1/2024/06/01/22-11-08-acf6f38e7cd8c712c7cb0c41b97b3962-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240601221027-6d9897.png#id=nkPEL&originHeight=731&originWidth=1300&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none)
上述图为**M-P神经元模型**``，在这个模型中，神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，然后通过**激活函数**处理以产生神经元的输出。
将**感知机**抽象为神经元，![](https://cdn.nlark.com/yuque/__latex/5b13ed0ae41bee9defcf75f2efc5f060.svg#card=math&code=x_i&id=VUfj7)为输入信号，![](https://cdn.nlark.com/yuque/__latex/c9b08ae6d9fed72562880f75720531bc.svg#card=math&code=w&id=S43yQ)为权重参数，![](https://cdn.nlark.com/yuque/__latex/bf98c0ddcbe9c1e535f767c78c3aa813.svg#card=math&code=y&id=nSV39) 通过激活函数进行阈值比较，输出兴奋度**（+1,-1）**。
而将多个神经元（感知机）按一定的层次结构连接起来，就得到了神经网络。在单层次下的感知机组成中，只能处理有限个线性可分平面下可分割的数据集，否则感知机组成学习过程会发生振荡，参数难以稳定。
![](https://cdn.jsdelivr.net/gh/xuezhaorong/Picgo//Source/fix-dir/Download/1/2024/06/02/11-07-20-dabe24d0471acf343073789afb940592-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240601223706-e4ba73.png#id=hp8eN&originHeight=906&originWidth=1645&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none)
所以出现了上图中的多层功能神经元，最简单的多层神经元结构中每层神经元与下一层神经元全互连接，神经元之间不存在同层连接，也不存在跨层连接，称之为**多层前馈神经网络（multi-layer feedforward neural networks）**。其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出，输入层神经元仅接收输入，不进行函数处理，隐层与输出层包含**功能神经元**。
神经网络的学习过程，就是根据训练数据来调整神经元之间的**连接权（connection weight）**以及每个功能神经元的阈值，换言之，神经网络学到的东西，蕴含在连接权与阈值中。
## 误差逆传播算法
对于多层前馈神经网络，简单感知机中用于更新参数的简单的梯度下降导数值无法适用了，因为它无法通过多层的神经元进行传递导数值。这时就要用到**误差逆传播算法（Error BackPropagation）或称为反向传播算法**对导数值进行按层传递。
![](https://cdn.jsdelivr.net/gh/xuezhaorong/Picgo//Source/fix-dir/Download/1/2024/06/01/23-59-04-fb282a614ff50175514a4012e2bb296a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240601235849-cc0b6b.png#id=tg5ee&originHeight=900&originWidth=1692&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none)
上图给定数据集![](https://cdn.nlark.com/yuque/__latex/32d7ff1dbf5cfedbc998c25a7ec4e233.svg#card=math&code=D%3D%5C%7B%28x_1%2Cy_1%29%2C%28x_2%2Cy_2%29%2C...%2C%28x_m%2Cy_m%29%5C%7D%2Cx_i%E2%88%88R%5Ed%2Cy_i%E2%88%88R%5El&id=By3ZK)，即输入示例由![](https://cdn.nlark.com/yuque/__latex/56c1b0cb7a48ccf9520b0adb3c8cb2e8.svg#card=math&code=d&id=L1QzN)个属性描述，输出![](https://cdn.nlark.com/yuque/__latex/6945e109777fe3fd777e8254f0ec0f0c.svg#card=math&code=l&id=ops5i)维实值向量，![](https://cdn.nlark.com/yuque/__latex/34c7b563b30bde3c748139530686798e.svg#card=math&code=q&id=qe6eO)个隐层神经元的多层前馈网络结构，其中输出层第![](https://cdn.nlark.com/yuque/__latex/036441a335dd85c838f76d63a3db2363.svg#card=math&code=j&id=lX2Kj)个神经元的阈值用![](https://cdn.nlark.com/yuque/__latex/91af6a1d9aa2a8903806c07d34d9b56d.svg#card=math&code=%CE%98_j&id=UKrRB)表示，隐层第![](https://cdn.nlark.com/yuque/__latex/67df0f404d0960fadcc99f6258733f22.svg#card=math&code=h&id=HmeSi)个神经元的阈值用![](https://cdn.nlark.com/yuque/__latex/441e1508e9cc054212ee84147cfa64bd.svg#card=math&code=%CE%B3_h&id=IHEfF)表示，输入层第![](https://cdn.nlark.com/yuque/__latex/036441a335dd85c838f76d63a3db2363.svg#card=math&code=j&id=unhaM)个神经元与隐层第![](https://cdn.nlark.com/yuque/__latex/67df0f404d0960fadcc99f6258733f22.svg#card=math&code=h&id=cgUPZ)个神经元之间的连接权维![](https://cdn.nlark.com/yuque/__latex/1eb01aec753b575050e035893d1d23ec.svg#card=math&code=v_ih&id=vJhvO)，隐层第![](https://cdn.nlark.com/yuque/__latex/67df0f404d0960fadcc99f6258733f22.svg#card=math&code=h&id=X1ddF)个神经元与输出层第![](https://cdn.nlark.com/yuque/__latex/036441a335dd85c838f76d63a3db2363.svg#card=math&code=j&id=QUpA0)个神经元之间的连接权为![](https://cdn.nlark.com/yuque/__latex/08f1be3acd2b170c74d1656da2e0747c.svg#card=math&code=w_%7Bhj%7D&id=oLhQ7)，记隐层第![](https://cdn.nlark.com/yuque/__latex/67df0f404d0960fadcc99f6258733f22.svg#card=math&code=h&id=XWelN)个神经元接收到的输入为![](https://cdn.nlark.com/yuque/__latex/08162943ab6caa8054c3cc2ea661cb89.svg#card=math&code=%CE%B1_h%3D%5Csum%5E%7Bd%7D_%7Bi%3D1%7Dv_%7Bih%7Dx_i&id=teXBZ)，输出层第![](https://cdn.nlark.com/yuque/__latex/036441a335dd85c838f76d63a3db2363.svg#card=math&code=j&id=dvvXw)个神经元接收道的输入为![](https://cdn.nlark.com/yuque/__latex/60a3d4f874261f7b24ca6ecd88520fec.svg#card=math&code=%CE%B2_j%3D%5Csum%5E%7Bq%7D_%7Bh%3D1%7Dw_%7Bhj%7Db_%7Bh%7D&id=LEsmp)，其中![](https://cdn.nlark.com/yuque/__latex/0441b24994e4fd5ebb5a4a3d8d52d2c5.svg#card=math&code=b_h&id=mRlVg)为隐层第![](https://cdn.nlark.com/yuque/__latex/67df0f404d0960fadcc99f6258733f22.svg#card=math&code=h&id=FXuYJ)个神经元的输出，假设隐层和输出层神经元都使用`Sigmoid`函数。
对训练例![](https://cdn.nlark.com/yuque/__latex/a050dbad1bc550cbffd030447ac79ea3.svg#card=math&code=%28x_k%2Cy_k%29&id=cvNaC)，假定神经网络的输出为![](https://cdn.nlark.com/yuque/__latex/ac8b4173bfca49fa48d311756d092f42.svg#card=math&code=%5Chat%7By_k%7D%3D%28%5Chat%7By%7D_1%5Ek%2C%5Chat%7By%7D_2%5Ek%2C...%2C%5Chat%7By%7D_l%5Ek%29&id=bKdSq)，![](https://cdn.nlark.com/yuque/__latex/2c2023102b99779003606a861202d0e8.svg#card=math&code=%5Chat%7By%7D_j%5Ek%3Df%28%CE%B2_j-%CE%B8_j%29&id=t2UG7)，则网络在![](https://cdn.nlark.com/yuque/__latex/bcd48409ba7825e747b34172304abad0.svg#card=math&code=%EF%BC%88x_k%2Cy_k%EF%BC%89&id=ptKgg)上的均方误差为：![](https://cdn.nlark.com/yuque/__latex/30378aab75e3507c95230d74cc276eaf.svg#card=math&code=E_k%3D%5Cfrac%7B1%7D%7B2%7D%5Csum%5E%7Bl%7D_%7Bj%3D1%7D%28%5Chat%7By%7D_j%5Ek-y_j%5Ek%29%5E2&id=dE8lT)
按照梯度下降的策略，以目标的负梯度方向对参数进行调整，得到![](https://cdn.nlark.com/yuque/__latex/35a6503778538f7a637c9931e9d514f3.svg#card=math&code=w_hj&id=mgiBY)的梯度值为：
![](https://cdn.nlark.com/yuque/__latex/664b9f06461810f855a2cf5fe7c53098.svg#card=math&code=%E2%88%87_%7Bw_%7Bhj%7D%7D%3D-%CE%B7%5Cfrac%7B%CF%83E_k%7D%7B%CF%83w_%7Bhj%7D%7D&id=KFrSW)
其中，![](https://cdn.nlark.com/yuque/__latex/08f1be3acd2b170c74d1656da2e0747c.svg#card=math&code=w_%7Bhj%7D&id=TCMZh)对![](https://cdn.nlark.com/yuque/__latex/fe614ad0636b7a9055842f74da955523.svg#card=math&code=%CE%B2&id=uA4s6)造成影响，![](https://cdn.nlark.com/yuque/__latex/c0a228082dc198ea52bd4c4cb2d253e9.svg#card=math&code=%CE%B2_j&id=lgGas)对![](https://cdn.nlark.com/yuque/__latex/5ed945db07b756cdbfa2ca62b1fdb0c4.svg#card=math&code=%5Chat%7By%5Ek_j%7D&id=ovSzg)造成影响，![](https://cdn.nlark.com/yuque/__latex/5ed945db07b756cdbfa2ca62b1fdb0c4.svg#card=math&code=%5Chat%7By%5Ek_j%7D&id=skwbc)对![](https://cdn.nlark.com/yuque/__latex/13a0eb555c0303e137d6aa98986c5277.svg#card=math&code=E_k&id=aRT3k)造成影响。
经过影响在神经元的传递，偏导公式可写为：
![](https://cdn.nlark.com/yuque/__latex/13e67eab5b5b0419016fbdc9d9f607de.svg#card=math&code=%5Cfrac%7B%CF%83E_k%7D%7B%CF%83w_%7Bhj%7D%7D%3D%5Cfrac%7B%CF%83E_k%7D%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%5Cfrac%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%7B%CF%83%CE%B2_%7Bj%7D%7D%5Cfrac%7B%CF%83%CE%B2_%7Bj%7D%7D%7B%CF%83w_%7Bhj%7D%7D&id=x3iMi)
对3个偏导分别求解：

1. ![](https://cdn.nlark.com/yuque/__latex/95c50c66f021a972b0dd5dbc0e13c6d3.svg#card=math&code=%5Cfrac%7B%CF%83%CE%B2_%7Bj%7D%7D%7B%CF%83w_%7Bhj%7D%7D%3Db_h&id=osgNX)
2. 由于`Sigmod`函数中，![](https://cdn.nlark.com/yuque/__latex/a151d502eb29df59967b69b19ff5a2ae.svg#card=math&code=f%5Cprime%28x%29%3Df%28x%29%281-f%28x%29%29&id=Frmpi)，所以![](https://cdn.nlark.com/yuque/__latex/d9e5b467186233bc2b0adbf3fa23c616.svg#card=math&code=g_j%3D%5Cfrac%7B%CF%83E_k%7D%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%5Cfrac%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%7B%CF%83%CE%B2_%7Bj%7D%7D%3D%28%5Chat%7By%7D_j%5Ek-y_j%5Ek%29f%5Cprime%28%CE%B2_j-%CE%B8_j%29%3D-%5Chat%7By%7D_j%5Ek%281-%5Chat%7By%7D_j%5Ek%29%28y_j%5Ek-%5Chat%7By%7D_j%5Ek%29&id=aq06H)

代入原式中得：![](https://cdn.nlark.com/yuque/__latex/cfa7b46444230b339e7d7bea04aec219.svg#card=math&code=%5Cfrac%7B%CF%83E_k%7D%7B%CF%83w_%7Bhj%7D%7D%3D%5Cfrac%7B%CF%83E_k%7D%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%5Cfrac%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%7B%CF%83%CE%B2_%7Bj%7D%7D%5Cfrac%7B%CF%83%CE%B2_%7Bj%7D%7D%7B%CF%83w_%7Bhj%7D%7D%3D-g_jb_h&id=UVO6H)
同样的：

- ![](https://cdn.nlark.com/yuque/__latex/776b10d287969545f32e42ba74561e76.svg#card=math&code=%5Cfrac%7B%CF%83E_k%7D%7B%CF%83%CE%B8_j%7D%3D%5Cfrac%7B%CF%83E_k%7D%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%5Cfrac%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%7B%CF%83%CE%B8_j%7D%3Dg_j&id=zwYR9)
- 对于![](https://cdn.nlark.com/yuque/__latex/49a95fde2de827afb5c636fca44f2756.svg#card=math&code=%5Cfrac%7B%CF%83E_k%7D%7B%CF%83v_%7Bih%7D%7D%3D%5Cfrac%7B%CF%83E_k%7D%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%5Cfrac%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%7B%CF%83%CE%B2_%7Bj%7D%7D%5Cfrac%7B%CF%83%CE%B2_%7Bj%7D%7D%7B%CF%83b_h%7D%5Cfrac%7B%CF%83b_h%7D%7B%CF%83%CE%B1_h%7D%5Cfrac%7B%CF%83%CE%B1_h%7D%7B%CF%83v_%7Bih%7D%7D%3D%5Cfrac%7B%CF%83E_%7Bk%7D%7D%7B%CF%83b_h%7D%5Cfrac%7B%CF%83b_%7Bh%7D%7D%7B%CF%83%CE%B1_h%7D%5Cfrac%7B%CF%83%CE%B1_h%7D%7B%CF%83v_%7Bih%7D%7D%3D-%5Csum_%7Bj-1%7D%5E%7Bl%7D%5Cfrac%7B%CF%83E_k%7D%7B%CF%83%CE%B2_%7Bj%7D%7D%5Cfrac%7B%CF%83%CE%B2_j%7D%7B%CF%83b_%7Bh%7D%7Df%5Cprime%28%CE%B1_h-%CE%B3_h%29x_i%3D-%5Csum_%7Bj%3D1%7D%5E%7Bl%7Dwh_jg_jf%5Cprime%28%CE%B1_h-%CE%B3_h%29%3D-b_h%281-b_h%29%5Csum_%7Bj%3D1%7D%5E%7Bl%7Dwh_jg_jx_i&id=sYk0Q)
- 令![](https://cdn.nlark.com/yuque/__latex/537353d4d5b810ee340d17866d0e8ceb.svg#card=math&code=eh%3D%5Csum_%7Bj%3D1%7D%5E%7Bl%7Dwh_jg_jf%5Cprime%28%CE%B1_h-%CE%B3_h%29%3Db_h%281-b_h%29%5Csum_%7Bj%3D1%7D%5E%7Bl%7Dwh_jg_j&id=MMxQI)，有![](https://cdn.nlark.com/yuque/__latex/41ea701e956b9eecfdf586399cd08804.svg#card=math&code=%5Cfrac%7B%CF%83E_k%7D%7B%CF%83%CE%B3_%7Bh%7D%7D%3D%5Cfrac%7B%CF%83E_k%7D%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%5Cfrac%7B%CF%83%5Chat%7By%7D_j%5Ek%7D%7B%CF%83%CE%B2_%7Bj%7D%7D%5Cfrac%7B%CF%83%CE%B2_%7Bj%7D%7D%7B%CF%83b_h%7D%5Cfrac%7B%CF%83b_h%7D%7B%CF%83%CE%B3_h%7D%3De_h&id=sBxbR)
## 算法流程
输入：训练集![](https://cdn.nlark.com/yuque/__latex/3f6fdf525ee4d4c9af4ade677eea75ba.svg#card=math&code=D%3D%7B%28x_k%2Cy_k%29%7D_%7Bk%3D1%7D%5E%7Bm%7D&id=HuJ9I)；学习率：`η`
过程：

1. 在（0，1）范围类随机初始化网络中所有连接权和阈值
2. repeat
3. fordo
4. 根据当前参数计算当前样本的输出
5. 计算输出层神经元的梯度项
6. 计算隐层神经元的梯度项
7. 更新连接权和阈值
8. end for
9. until 达到停止条件

输出：连接权于阈值确定的多层前馈神经网络
![](https://cdn.jsdelivr.net/gh/xuezhaorong/Picgo//Source/fix-dir/Download/1/2024/06/02/12-48-13-cc412a248576cd3f2c74fc8284ebb5d8-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240602124802-5aad60.png#id=VFy3M&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none)
## 代码实现
```python
class MultilayerPerceptron:
    def __init__(self, data, labels, layers, normalize_data=False):
        data_processed = prepare_for_training(data, normalize_data=normalize_data)[0]
        self.data = data_processed
        self.labels = labels
        # 神经网络的层数
        self.layers = layers  # 输入层 隐层 输出层
        self.normalize_data = normalize_data
        self.thetas = MultilayerPerceptron.thetas_init(layers)

    def train(self, max_iterations=1000, alpha=0.1):
        unrolled_theta = MultilayerPerceptron.thetas_unroll(self.thetas)
        # 梯度下降
        (optimized_theta, cost_history) = MultilayerPerceptron.gradient_descent(self.data,
                                                                                self.labels,
                                                                                unrolled_theta,
                                                                                self.layers, max_iterations,
                                                                                alpha)
        self.thetas = MultilayerPerceptron.thetas_roll(optimized_theta, self.layers)
        return self.thetas, cost_history

    @staticmethod
    def thetas_init(layers):
        # 层的个数
        num_layers = len(layers)
        thetas = {}
        for layer_index in range(num_layers - 1):
            # 输入
            in_count = layers[layer_index]
            # 输出
            out_count = layers[layer_index + 1]
            # 随机赋值
            thetas[layer_index] = np.random.rand(out_count, in_count + 1)  # 添加偏置项
        return thetas

    @staticmethod
    def thetas_unroll(thetas):
        num_theta_layers = len(thetas)
        unrolled_theta = np.array([])
        for theta_layer_index in range(num_theta_layers):
            unrolled_theta = np.hstack((unrolled_theta, thetas[theta_layer_index].flatten()))
        return unrolled_theta

    @staticmethod
    def thetas_roll(unrolled_thetas, layers):
        num_layers = len(layers)
        thetas = {}
        # 标识符
        unrolled_shift = 0
        for layer_index in range(num_layers - 1):
            in_count = layers[layer_index]
            out_count = layers[layer_index + 1]
            thetas_width = in_count + 1
            thetas_height = out_count
            thetas_volume = thetas_width * thetas_height
            start_index = unrolled_shift
            end_index = start_index + thetas_volume
            layer_theta_unrolled = unrolled_thetas[start_index:end_index]
            thetas[layer_index] = layer_theta_unrolled.reshape((thetas_height, thetas_width))
            unrolled_shift = unrolled_shift + thetas_volume
        return thetas

    # 损失函数
    @staticmethod
    def cost_function(data, labels, thetas, layers):
        num_layers = len(layers)
        num_examples = data.shape[0]
        num_labels = layers[-1]

        # 进行前向传播
        predictions = MultilayerPerceptron.feedforward_propagation(data, thetas, layers)
        # 制作标签
        bitwise_labels = np.zeros((num_examples, num_labels))
        for example_index in range(num_examples):
            bitwise_labels[example_index][labels[example_index][0]] = 1
        # 利用交叉熵计算损失值
        bit_set_cost = np.sum(np.log(predictions[bitwise_labels == 1]))
        bit_not_set_cost = np.sum(np.log(1 - predictions[bitwise_labels == 0]))
        cost = (-1 / num_examples) * (bit_set_cost + bit_not_set_cost)
        return cost

    @staticmethod
    def gradient_descent(data, labels, unrolled_theta, layers, max_iterations, alpha):
        # 最优theta
        optimized_theta = unrolled_theta
        # 损失值
        cost_history = []
        for _ in range(max_iterations):
            cost = MultilayerPerceptron.cost_function(data,
                                                      labels,
                                                      MultilayerPerceptron.thetas_roll(optimized_theta, layers),
                                                      layers)
            cost_history.append(cost)
            # 更新theta
            theta_gradient = MultilayerPerceptron.gradient_step(data, labels, optimized_theta, layers)
            optimized_theta = optimized_theta - alpha * theta_gradient
        return optimized_theta, cost_history

    # 前向传播
    @staticmethod
    def feedforward_propagation(data, thetas, layers):
        num_layers = len(layers)
        num_examples = data.shape[0]
        in_layer_activation = data

        # 逐层计算
        for layer_index in range(num_layers - 1):
            theta = thetas[layer_index]
            # out = x * θ.T
            out_layer_activation = sigmoid(np.dot(in_layer_activation, theta.T))
            # 加上偏置项
            out_layer_activation = np.hstack((np.ones((num_examples, 1)), out_layer_activation))
            # 更新输入
            in_layer_activation = out_layer_activation
        # 返回输出层结果
        return in_layer_activation[:, 1:]

    # 后向传播
    @staticmethod
    def back_propagation(data, labels, thetas, layers):
        num_layers = len(layers)
        (num_examples, num_features) = data.shape
        num_label_type = layers[-1]
        # 每一层的梯度
        deltas = {}
        # 初始化deltas
        for layer_index in range(num_layers - 1):
            in_count = layers[layer_index]
            out_count = layers[layer_index+1]
            deltas[layer_index] = np.zeros((out_count, in_count+1))

        for example_index in range(num_examples):
            layers_inputs = {}
            layers_activations = {}
            layers_activation = data[example_index, :].reshape((num_features, 1))
            # 第1层的输入为data
            layers_activations[0] = layers_activation

            # 前向传播算出输出层的值
            for layer_index in range(num_layers-1):
                # 得到当前权重参数
                layer_theta = thetas[layer_index]
                layer_input = np.dot(layer_theta, layers_activation)
                layers_activation = np.vstack((np.array([[1]]), sigmoid(layer_input)))
                layers_inputs[layer_index + 1] = layer_input
                layers_activations[layer_index + 1] = layers_activation
            output_layer_activation = layers_activation[1:, :]

            delta = {}
            bitwise_label = np.zeros((num_label_type, 1))
            bitwise_label[labels[example_index][0]] = 1
            # 计算输出层和真实值之间的差异
            delta[num_layers - 1] = output_layer_activation - bitwise_label

            # 反向传播算出每一层的误差差异 δ_{k-1} = θ.T*δ_k*sigmoid_gradient
            for layer_index in range(num_layers - 2, 0, -1):
                layer_theta = thetas[layer_index]
                next_delta = delta[layer_index+1]
                layer_input = layers_inputs[layer_index]
                layer_input = np.vstack((np.array(1), layer_input))
                delta[layer_index] = np.dot(layer_theta.T, next_delta) * sigmoid_gradient(layer_input)
                # 过滤偏置参数
                delta[layer_index] = delta[layer_index][1:, :]

            # 计算梯度值 gradient_{k-1} = gradient_k + δ_k*a_k
            for layer_index in range(num_layers - 1):
                layer_delta = np.dot(delta[layer_index+1], layers_activations[layer_index].T)
                deltas[layer_index] = deltas[layer_index] + layer_delta
        # 求平均
        for layer_index in range(num_layers - 1):
            deltas[layer_index] = deltas[layer_index] * (1/num_examples)

        return deltas

    @staticmethod
    def gradient_step(data, labels, optimized_data, layers):
        theta = MultilayerPerceptron.thetas_roll(optimized_data, layers)
        thetas_rolled_gradients = MultilayerPerceptron.back_propagation(data, labels, theta, layers)
        thetas_unrolled_gradients = MultilayerPerceptron.thetas_unroll(thetas_rolled_gradients)
        return thetas_unrolled_gradients

```
